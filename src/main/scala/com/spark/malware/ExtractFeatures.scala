package com.spark.malware

import java.io.File

import org.apache.hadoop.fs.{FileSystem, LocatedFileStatus, Path, RemoteIterator}
import org.apache.spark.rdd.RDD

import scala.collection.mutable.ListBuffer

/**
  * Created by ram on 11/23/16.
  */

object FileSystemWrapper {

  def getFiles( inputDirectoryName : String ) = {

    val files = FileSystem.get(RunParameters.sparkContext.hadoopConfiguration).listFiles(new Path(inputDirectoryName), true)
    var inputFiles = ListBuffer[String]()

    while (files.hasNext) {
      val file = files.next
      inputFiles += file.getPath.toString
    }

    inputFiles.toList
  }

}


object ExtractFeatures {

  def extractNgrams( inputDirectoryName : String ) : RDD[(String, String)] = {

    val files = FileSystemWrapper.getFiles(inputDirectoryName)
    var ngramsCollection: org.apache.spark.rdd.RDD[(String, String)] = RunParameters.sparkContext.emptyRDD

    for (file <- files) {

      val filecontent = RunParameters.sparkContext.textFile(file)
      val ngrams = filecontent.map(row => row.replace(row.split(Constants.SYMBOL_SPACE)(0), Constants.EMPTY_STRING).toString).filter(row => (!row.contains("??"))).map {
        _.split(Constants.SYMBOL_DOT).map { substring => substring.trim.split(Constants.SYMBOL_SPACE).sliding(Constants.N_GRAM_SIZE) }.flatMap {
          identity
        }.map {
          _.mkString(Constants.SYMBOL_SPACE)
        }.groupBy {
          identity
        }.mapValues {
          _.size
        }
      }.flatMap {
        identity
      }.reduceByKey(_ + _).sortBy(_._2)

      val ngramsKeyValuePair = ngrams.map(x => (x._1.split(Constants.SYMBOL_SPACE)
        .mkString(Constants.EMPTY_STRING), file.substring(file.lastIndexOf("/")+1, file.length) + Constants.SYMBOL_COLON + x._2 + Constants.SYMBOL_COLON + ( if ( RunParameters.fileTypeSet(file.substring(file.lastIndexOf("/")+1, file.length) ) ) 1 else 0 )   ))
      ngramsCollection = ngramsCollection.union(ngramsKeyValuePair)
    }

    ngramsCollection
  }
}
