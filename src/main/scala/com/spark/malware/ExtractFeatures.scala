package com.spark.malware

import java.io.File
import org.apache.hadoop.fs.{FileSystem, Path}

import org.apache.spark.rdd.RDD

/**
  * Created by ram on 11/23/16.
  */
object ExtractFeatures {

  def extractNgrams( inputDirectoryName : String ) : RDD[(String, String)] = {

    println(inputDirectoryName)
    val files = new java.io.File(inputDirectoryName).listFiles
    //val files = FileSystem.get(DetectMalwareController.sparkContext.hadoopConfiguration).listStatus(new Path(inputDirectoryName))
    var ngramsCollection: org.apache.spark.rdd.RDD[(String, String)] = DetectMalwareController.sparkContext.emptyRDD

    for (file <- files) {
      println(file)
      val filecontent = DetectMalwareController.sparkContext.textFile(file.getAbsolutePath())
      val ngrams = filecontent.map(row => row.replace(row.split("\\s")(0), "").toString).filter(row => (!row.contains("??"))).map {
        _.split('.').map { substring => substring.trim.split(" ").sliding(4) }.flatMap {
          identity
        }.map {
          _.mkString(" ")
        }.groupBy {
          identity
        }.mapValues {
          _.size
        }
      }.flatMap {
        identity
      }.reduceByKey(_ + _).sortBy(_._2)

      val ngramsKeyValuePair = ngrams.map(x => (x._1.split("\\s").mkString(""), file.getName() + ":" + x._2 + ":" + 1))
      ngramsCollection = ngramsCollection.union(ngramsKeyValuePair)
    }
    ngramsCollection
  }
}
