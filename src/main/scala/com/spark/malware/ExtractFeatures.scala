package com.spark.malware

import java.io.File
import org.apache.hadoop.fs.{FileSystem, Path}

import org.apache.spark.rdd.RDD

/**
  * Created by ram on 11/23/16.
  */
object ExtractFeatures {

  def extractNgrams( inputDirectoryName : String ) : RDD[(String, String)] = {

    val files = new java.io.File(inputDirectoryName).listFiles
    var ngramsCollection: org.apache.spark.rdd.RDD[(String, String)] = RunParameters.sparkContext.emptyRDD
    /*val files = FileSystem.get(RunParameters.sparkContext.hadoopConfiguration).listFiles(new Path(inputDirectoryName), true)
    var ngramsCollection: org.apache.spark.rdd.RDD[(String, String)] = RunParameters.sparkContext.emptyRDD

    var filesPathString = ""
    var filesString = ""

    while( files.hasNext ) {
      val file = files.next()
      filesString += file.getPath.getName + ","
      filesPathString += file.getPath.toString + ","
      println(filesPathString)
    }

    val fileNameList = RunParameters.sparkContext.broadcast(filesString.split(",").toList).value
    println(fileNameList)
    val filePathList = RunParameters.sparkContext.broadcast(filesPathString.split(",").toList).value
    println(filePathList)

    for ( i<-0 to  filePathList.length-1) {
      val fileName = fileNameList(i)
      val filePath = filePathList(i)
      val filecontent = RunParameters.sparkContext.textFile(filePath)
      val ngrams = filecontent.map(row => row.replace(row.split("\\s")(0), "").toString).filter(row => (!row.contains("??"))).map {
        _.split('.').map { substring => substring.trim.split(" ").sliding(Constants.N_GRAM_SIZE) }.flatMap {
          identity
        }.map {
          _.mkString(" ")
        }.groupBy {
          identity
        }.mapValues {
          _.size
        }
      }.flatMap {
        identity
      }.reduceByKey(_ + _).sortBy(_._2)

      val ngramsKeyValuePair = ngrams
        .map(x =>
          (x._1.split("\\s").mkString(""), fileName + ":" + x._2 + ":" + (if (RunParameters.fileTypeSet(fileName)) 1 else 0))
        )
      ngramsCollection = ngramsCollection.union(ngramsKeyValuePair)
    }
    */

    for (file<-files) {
      val filecontent = RunParameters.sparkContext.textFile(file.getPath)
      val ngrams = filecontent.map(row => row.replace(row.split(Constants.SYMBOL_SPACE)(0), Constants.EMPTY_STRING).toString).filter(row => (!row.contains("??"))).map {
        _.split(Constants.SYMBOL_DOT).map { substring => substring.trim.split(Constants.SYMBOL_SPACE).sliding(Constants.N_GRAM_SIZE) }.flatMap {
          identity
        }.map {
          _.mkString(Constants.SYMBOL_SPACE)
        }.groupBy {
          identity
        }.mapValues {
          _.size
        }
      }.flatMap {
        identity
      }.reduceByKey(_ + _).sortBy(_._2)

      val ngramsKeyValuePair = ngrams.map(x => (x._1.split(Constants.SYMBOL_SPACE).mkString(Constants.EMPTY_STRING), file.getName + Constants.SYMBOL_COLON + x._2 + Constants.SYMBOL_COLON + ( if ( RunParameters.fileTypeSet(file.getName) ) 1 else 0 )   ))
      ngramsCollection = ngramsCollection.union(ngramsKeyValuePair)
    }

    ngramsCollection
  }
}
