package com.spark.malware

import org.apache.spark.SparkContext
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.rdd.RDD

/**
  * Created by sparknode on 11/11/16.
  */
object Utilities {

  def getRDD(filename: String, sparkContext: SparkContext):RDD[String] = {
    sparkContext.textFile(filename)
  }

  def getDataSplit( data: RDD[LabeledPoint] ) = {

    val Array(training, test) = data.randomSplit(Array(Constants.DATA_SPLIT_TRAINING,
      Constants.DATA_SPLIT_TESTING))

    Array(training, test)
  }

  def convertDataToLabeledPoints( records : RDD[(String, String)] ) = {

    val parsedData = records.map( line => getLabeledPoint(line._2) )

    parsedData
  }

  def getLabeledPoint( line : String ) = {

    val originalFeatureListWithLabel = line.split(Constants.SYMBOL_COMMA)
    val label = originalFeatureListWithLabel(originalFeatureListWithLabel.length-1)
    val featureList = originalFeatureListWithLabel.dropRight(1)
    val vectorList = Vectors.dense(featureList.map(_.toDouble))
    val labeledPoint = new LabeledPoint(label.toDouble, vectorList)

    labeledPoint

  }

  def loadDatasetDetails( fileName: String ) = {

    Constants.MALICIOUS_FILES = RunParameters.sparkContext.textFile(fileName)
      .map( record => (record.split(Constants.SYMBOL_COMMA)(0), record.split(Constants.SYMBOL_COMMA)(1)) )
      .filter( record => record._2.equals(Constants.CLASS_ONE) )
      .map( record => record._1+Constants.DATASET_EXTN.trim)
      .collect()

  }

}
